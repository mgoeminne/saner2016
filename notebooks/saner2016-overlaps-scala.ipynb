{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to convert first-last.summary.csv file into relative coverage values over X regularly time spaced steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.11.6;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 10:12:58 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.11.6;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.11.6;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 11:11:45 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.11.6;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.11;0.2.0-SNAPSHOT from sonatype-snapshots, using Mon Jun 01 02:54:01 CEST 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.11;0.2.0-SNAPSHOT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.ivy(\"joda-time\" % \"joda-time\" % \"2.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.io.{PrintWriter, File}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.joda.time.{DateTime, DateTimeZone, Duration, Interval}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.joda.time.format.DateTimeFormat\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.collection.immutable.TreeMap\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "import java.io.{PrintWriter, File}\n",
    "import org.joda.time.{DateTime, DateTimeZone, Duration, Interval}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import scala.collection.immutable.TreeMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdirectory\u001b[0m: \u001b[32mFile\u001b[0m = /Users/mg/CloudStation/publications/2016/saner2016/data\n",
       "\u001b[36mhistory_file\u001b[0m: \u001b[32mFile\u001b[0m = /Users/mg/CloudStation/publications/2016/saner2016/data/first-last.summary.csv\n",
       "\u001b[36mheaders\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"project\"\u001b[0m, \u001b[32m0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"file\"\u001b[0m, \u001b[32m1\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"jpa_first\"\u001b[0m, \u001b[32m2\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"jpa_last\"\u001b[0m, \u001b[32m3\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"jdbc_first\"\u001b[0m, \u001b[32m4\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"jdbc_last\"\u001b[0m, \u001b[32m5\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"hbm_first\"\u001b[0m, \u001b[32m6\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"hbm_last\"\u001b[0m, \u001b[32m7\u001b[0m)\n",
       ")\n",
       "\u001b[36mdtf\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mjoda\u001b[0m.\u001b[32mtime\u001b[0m.\u001b[32mformat\u001b[0m.\u001b[32mDateTimeFormatter\u001b[0m = org.joda.time.format.DateTimeFormatter@5115a68b\n",
       "\u001b[36mNB_STEPS\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m10\u001b[0m\n",
       "defined \u001b[32mfunction \u001b[36morderingByDateTime\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mRow\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mRow\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Change the directory and/or the used files for adapting this script to your configuration\n",
    "val directory = new File(\"/Users/mg/CloudStation/publications/2016/saner2016/data\")\n",
    "val history_file = new File(directory, \"first-last.summary.csv\")\n",
    "val headers = Source.fromFile(history_file).getLines.next split \",\" zipWithIndex\n",
    "val dtf = DateTimeFormat.forPattern(\"yyyy-MM-dd HH:mm:ss\").withZoneUTC\n",
    "\n",
    "val NB_STEPS = 10\n",
    "\n",
    "implicit def orderingByDateTime[A <: DateTime]: Ordering[A] = Ordering.by(d => d.getMillis)\n",
    "\n",
    "case class Row(project: String, \n",
    "               file: String,\n",
    "               jpa: Option[Interval], \n",
    "               jdbc: Option[Interval],\n",
    "               hbm: Option[Interval])\n",
    "{\n",
    "    def is_entity_related(): Boolean = jpa.isDefined || jdbc.isDefined || hbm.isDefined\n",
    "    \n",
    "    def dates: Seq[DateTime] =\n",
    "    {\n",
    "        var ret = Seq[DateTime]()\n",
    "        ret = if(jpa.isDefined)  ret ++ Seq(jpa.get.getStart,jpa.get.getEnd) else ret\n",
    "        ret = if(jdbc.isDefined) ret ++ Seq(jdbc.get.getStart,jdbc.get.getEnd) else ret\n",
    "        ret = if(hbm.isDefined)  ret ++ Seq(hbm.get.getStart,hbm.get.getEnd) else ret\n",
    "        \n",
    "        ret\n",
    "    }\n",
    "    \n",
    "    def is_jpa(t: DateTime): Boolean = jpa match {\n",
    "        case s: Some[Interval] => !((t isBefore s.get.getStart) || (t isAfter s.get.getEnd))\n",
    "        case _ => false\n",
    "    }\n",
    "    \n",
    "    def is_jdbc(t: DateTime): Boolean = jdbc match {\n",
    "        case s: Some[Interval] => !((t isBefore s.get.getStart) || (t isAfter s.get.getEnd))\n",
    "        case _ => false\n",
    "    }\n",
    "    \n",
    "    def is_hbm(t: DateTime): Boolean = hbm match {\n",
    "        case s: Some[Interval] => !((t isBefore s.get.getStart) || (t isAfter s.get.getEnd))\n",
    "        case _ => false\n",
    "    }\n",
    "}\n",
    "\n",
    "object Row\n",
    "{\n",
    "    def apply(line: String) = {\n",
    "        val elements = line.split(\",\", -1)\n",
    "        val project = elements(0)\n",
    "        val file = elements(1)\n",
    "        val jpa = elements(2) match {\n",
    "            case \"NA\" => None\n",
    "            case \"\"   => None\n",
    "            case _: String => Some(new Interval( dtf.parseDateTime(elements(2)) , dtf.parseDateTime(elements(3))))  \n",
    "        }\n",
    "        \n",
    "        val jdbc = elements(4) match {\n",
    "            case \"NA\" => None\n",
    "            case \"\"   => None\n",
    "            case _: String => Some(new Interval(dtf.parseDateTime(elements(4)) , dtf.parseDateTime(elements(5))))  \n",
    "        }\n",
    "        \n",
    "        val hbm = elements(6) match {\n",
    "            case \"NA\" => None\n",
    "            case \"\"   => None\n",
    "            case _: String => Some(new Interval(dtf.parseDateTime(elements(6)) , dtf.parseDateTime(elements(7))))  \n",
    "        }\n",
    "        \n",
    "        \n",
    "        new Row(project,file,jpa,jdbc,hbm)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mentries\u001b[0m: \u001b[32mList\u001b[0m[\u001b[32mRow\u001b[0m] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33mRow\u001b[0m(\n",
       "    \u001b[32m\"codjo_codjo-sample\"\u001b[0m,\n",
       "    \u001b[32m\"sample-server/src/main/java/net/codjo/sample/server/broadcast/ComputedPublicationDate.java\"\u001b[0m,\n",
       "    None,\n",
       "    Some(2012-07-09T14:35:56.000Z/2014-09-17T11:43:58.000Z),\n",
       "    None\n",
       "  ),\n",
       "  \u001b[33mRow\u001b[0m(\n",
       "    \u001b[32m\"codjo_codjo-sample\"\u001b[0m,\n",
       "    \u001b[32m\"sample-server/src/main/java/net/codjo/sample/server/broadcast/BookSelector.java\"\u001b[0m,\n",
       "    None,\n",
       "    Some(2012-12-18T18:18:47.000Z/2014-09-17T11:43:58.000Z),\n",
       "    None\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mprojects\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mList\u001b[0m[\u001b[32mRow\u001b[0m]] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m\"Netflix_astyanax\"\u001b[0m -> \u001b[33mList\u001b[0m(\n",
       "    \u001b[33mRow\u001b[0m(\n",
       "      \u001b[32m\"Netflix_astyanax\"\u001b[0m,\n",
       "      \u001b[32m\"src/test/java/com/netflix/astyanax/entitystore/DoubleIdColumnEntity.java\"\u001b[0m,\n",
       "      Some(2013-01-28T10:07:04.000Z/2013-02-11T09:47:00.000Z),\n",
       "      None,\n",
       "      None\n",
       "    ),\n",
       "    \u001b[33mRow\u001b[0m(\n",
       "      \u001b[32m\"Netflix_astyanax\"\u001b[0m,\n",
       "      \u001b[32m\"src/test/java/com/netflix/astyanax/entitystore/TtlEntity.java\"\u001b[0m,\n",
       "      Some(2013-01-28T10:07:04.000Z/2013-02-11T09:47:00.000Z),\n",
       "      None,\n",
       "      None\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val entries = Source.fromFile(history_file)\n",
    "                    .getLines.drop(1)\n",
    "                    .filterNot(_.trim.isEmpty)\n",
    "                    .map(Row.apply)\n",
    "                    .toList\n",
    "val projects = entries.groupBy(_.project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2228 projects and 93842 entries\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(projects.size + \" projects and \" + entries.size + \" entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes on file the evolution (step by step) of technology coverage of in projects.\n",
    "The output file present several kind of values, some of them depending on technology T:\n",
    " - project, the project name\n",
    " - step, the relative step of project's life. Between 0 (first observed commit) and 1 (last observed commit)\n",
    " - entities: number of files covered by at least 1 technology\n",
    " - T-all: number of files covered at least by *T*\n",
    " - relative-T-all: T-all / entities\n",
    " - T: number of files only covered by *T*\n",
    " - relative-T: number of files only covered by *T*, divided by *entities*\n",
    " - T1-T2: number of files only covered by *T1* and *T2*\n",
    " - relative-T1-T2: relative number of files covered by *T1* and *T2*. Between 0 (no coverage by this pair) and 1 (all the files covered by *T1* are also covered by *T2*, and reciprocally. Computed as (T1-T2)/max(T1-all, T2-all). Value is 0 if there is no file covered by *T1* nor *T2*.\n",
    " - T1-T2-T3: number of files covered by the 3 considered technologies\n",
    " - relative-T1-T2-T3: relative number of files covered by the 3 considered technologies. Between 0 (no coverage by this triplet) and 1 (each covered file is covered by the 3 technologies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moutput\u001b[0m: \u001b[32mPrintWriter\u001b[0m = java.io.PrintWriter@721f852e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val output = new PrintWriter(new File(directory, \"techno-coverage.csv\"))\n",
    "output.println(\"project,step,entities,jpa-all,relative-jpa-all,jdbc-all,relative-jdbc-all,hbm-all,relative-hbm-all,jpa,relative-jpa,jdbc,relative-jdbc,hbm,relative-hbm,jpa-jdbc,relative-jpa-jdbc,jpa-hbm,relative-jpa-hbm,jdbc-hbm,relative-jdbc-hbm,jpa-jdbc-hbm,relative-jpa-jdbc-hbm\")\n",
    "\n",
    "projects.foreach(p => {\n",
    "    val dates = p._2.flatMap(r => r.dates)\n",
    "    val min_date = dates.minBy(d => d.getMillis)\n",
    "    val max_date = dates.maxBy(d => d.getMillis)\n",
    "    val interval = new Duration(min_date,max_date).getStandardSeconds / NB_STEPS\n",
    "    \n",
    "    // Divide project history into regularly spaced datetime\n",
    "    val steps = (0 to NB_STEPS) map (s => s -> (min_date plus Duration.standardSeconds(s*interval)))\n",
    "    \n",
    "    steps.foreach(step => {\n",
    "        val contains_jpa  = p._2 filter (r => r.is_jpa(step._2)) toSet\n",
    "        val contains_jdbc = p._2 filter (r => r.is_jdbc(step._2)) toSet\n",
    "        val contains_hbm  = p._2 filter (r => r.is_hbm(step._2)) toSet\n",
    "                \n",
    "        // Number of files covered by a technology\n",
    "        val nb_entities = (contains_jpa union contains_jdbc union contains_hbm) size\n",
    "        \n",
    "        // Number of files covered by at least this technology\n",
    "        val nb_jpa_all = contains_jpa size  \n",
    "        val nb_relative_jpa_all = nb_jpa_all.toFloat / math.max(nb_entities, 1)\n",
    "        \n",
    "        val nb_jdbc_all = contains_jdbc size\n",
    "        val nb_relative_jdbc_all = nb_jdbc_all.toFloat / math.max(nb_entities, 1)\n",
    "        \n",
    "        val nb_hbm_all = contains_hbm size\n",
    "        val nb_relative_hbm_all = nb_hbm_all.toFloat / math.max(nb_entities, 1)\n",
    "        \n",
    "        // Number of files covered by this technology only\n",
    "        val nb_jpa  = ((contains_jpa diff contains_jdbc) diff contains_hbm) size\n",
    "        val nb_relative_jpa = nb_jpa.toFloat / math.max(nb_entities, 1)\n",
    "        \n",
    "        val nb_jdbc = ((contains_jdbc diff contains_jpa) diff contains_hbm) size\n",
    "        val nb_relative_jdbc = nb_jdbc.toFloat / math.max(nb_entities, 1)\n",
    "        \n",
    "        val nb_hbm  = ((contains_hbm diff contains_jpa) diff contains_jdbc) size\n",
    "        val nb_relative_hbm = nb_hbm.toFloat / math.max(nb_entities, 1)\n",
    "        \n",
    "        // Number of files covered by a pair of technologies only\n",
    "        val nb_jpa_jdbc = ((contains_jpa intersect contains_jdbc) diff contains_hbm) size\n",
    "        val nb_relative_jpa_jdbc = nb_jpa_jdbc.toFloat / Seq(nb_jpa_all, nb_jdbc_all, 1).max\n",
    "        \n",
    "        val nb_jpa_hbm  = ((contains_jpa intersect contains_hbm) diff contains_jdbc) size\n",
    "        val nb_relative_jpa_hbm = nb_jpa_hbm.toFloat / Seq(nb_jpa_all, nb_hbm_all, 1).max\n",
    "        \n",
    "        val nb_jdbc_hbm = ((contains_jdbc intersect contains_hbm) diff contains_jpa) size\n",
    "        val nb_relative_jdbc_hbm = nb_jdbc_hbm.toFloat / Seq(nb_jdbc_all, nb_hbm_all, 1).max\n",
    "        \n",
    "        // Number of files covered by the 3 considered technologies\n",
    "        val nb_jpa_jdbc_hbm = (contains_jpa intersect contains_jdbc intersect contains_hbm) size\n",
    "        val nb_relative_jpa_jdbc_hbm = nb_jpa_jdbc_hbm.toFloat / Seq(nb_jpa_all, nb_jdbc_all, nb_hbm_all, 1).max\n",
    "        \n",
    "        assert(nb_entities == (\n",
    "            nb_jpa + nb_jdbc + nb_hbm +\n",
    "            nb_jpa_jdbc + nb_jpa_hbm + nb_jdbc_hbm +\n",
    "            nb_jpa_jdbc_hbm\n",
    "        ))\n",
    "        \n",
    "        output.println(p._1 + \",\" + step._1.toFloat / NB_STEPS + \",\" + \n",
    "                       \n",
    "                       nb_entities + \",\" + \n",
    "                       nb_jpa_all + \",\" +\n",
    "                       nb_relative_jpa_all + \",\" +\n",
    "                       \n",
    "                       nb_jdbc_all + \",\" + \n",
    "                       nb_relative_jdbc_all + \",\" +\n",
    "                       \n",
    "                       nb_hbm_all + \",\" + \n",
    "                       nb_relative_hbm_all + \",\" +\n",
    "                       \n",
    "                       nb_jpa + \",\" + \n",
    "                       nb_relative_jpa + \",\" + \n",
    "                       \n",
    "                       nb_jdbc + \",\" + \n",
    "                       nb_relative_jdbc + \",\" + \n",
    "                       \n",
    "                       nb_hbm + \",\" + \n",
    "                       nb_relative_hbm + \",\" + \n",
    "                       \n",
    "                       nb_jpa_jdbc + \",\" +\n",
    "                       nb_relative_jpa_jdbc + \",\" +\n",
    "                       \n",
    "                       nb_jpa_hbm + \",\" +\n",
    "                       nb_relative_jpa_hbm + \",\" +\n",
    "                       \n",
    "                       nb_jdbc_hbm + \",\" + \n",
    "                       nb_relative_jdbc_hbm + \",\" +\n",
    "                       \n",
    "                       nb_jpa_jdbc_hbm + \",\" +\n",
    "                       nb_relative_jpa_jdbc_hbm   \n",
    "                      )\n",
    "    })\n",
    "    \n",
    "})\n",
    "\n",
    "output.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moutput\u001b[0m: \u001b[32mPrintWriter\u001b[0m = java.io.PrintWriter@7802b9be"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Writes on file a flat representation of the evolution (step by step) of technology coverage in projects\n",
    "\n",
    "val output = new PrintWriter(new File(directory, \"flat-techno-coverage.csv\"))\n",
    "\n",
    "output.println(\"project,\" + (0 to NB_STEPS).map (s => \"entities-\" + s + \n",
    "                                                      \",jpa-\" + s +\n",
    "                                                      \",jdbc-\" + s +\n",
    "                                                      \",hbm-\" + s +\n",
    "                                                      \",jpa-jdbc-\" + s +\n",
    "                                                      \",jpa-hbm-\" + s +\n",
    "                                                      \",jdbc-hbm-\" + s +\n",
    "                                                      \",jpa-jdbc-hbm-\" + s\n",
    "                                                ).mkString(\",\"))\n",
    "\n",
    "projects.foreach(p => {\n",
    "    val dates = p._2.flatMap(r => r.dates)\n",
    "    val min_date = dates.minBy(d => d.getMillis)\n",
    "    val max_date = dates.maxBy(d => d.getMillis)\n",
    "    val interval = new Duration(min_date,max_date).getStandardSeconds / NB_STEPS\n",
    "    \n",
    "    output.print(p._1 + \",\")\n",
    "    \n",
    "    // Divide project history into regularly spaced datetime\n",
    "    val steps = (0 to NB_STEPS) map (s => s -> (min_date plus Duration.standardSeconds(s*interval)))\n",
    "    \n",
    "    output.println(steps.map(step => {\n",
    "        val contains_jpa  = p._2 filter (r => r.is_jpa(step._2)) toSet\n",
    "        val contains_jdbc = p._2 filter (r => r.is_jdbc(step._2)) toSet\n",
    "        val contains_hbm  = p._2 filter (r => r.is_hbm(step._2)) toSet\n",
    "        \n",
    "        val nb_entities = (contains_jpa union contains_jdbc union contains_hbm) size\n",
    "        \n",
    "        val nb_jpa  = ((contains_jpa diff contains_jdbc) diff contains_hbm) size\n",
    "        val nb_jdbc = ((contains_jdbc diff contains_jpa) diff contains_hbm) size\n",
    "        val nb_hbm  = ((contains_hbm diff contains_jpa) diff contains_jdbc) size\n",
    "        \n",
    "        val nb_jpa_jdbc = ((contains_jpa intersect contains_jdbc) diff contains_hbm) size\n",
    "        val nb_jpa_hbm  = ((contains_jpa intersect contains_hbm) diff contains_jdbc) size\n",
    "        val nb_jdbc_hbm = ((contains_jdbc intersect contains_hbm) diff contains_jpa) size\n",
    "        \n",
    "        val nb_jpa_jdbc_hbm = (contains_jpa intersect contains_jdbc intersect contains_hbm) size\n",
    "        \n",
    "        assert(nb_entities == (\n",
    "            nb_jpa + nb_jdbc + nb_hbm +\n",
    "            nb_jpa_jdbc + nb_jpa_hbm + nb_jdbc_hbm +\n",
    "            nb_jpa_jdbc_hbm\n",
    "        ))\n",
    "        \n",
    "        (nb_entities + \",\" + \n",
    "                nb_jpa.toFloat / math.max(nb_entities,1) + \",\" + \n",
    "                nb_jdbc.toFloat / math.max(nb_entities,1) + \",\" + \n",
    "                nb_hbm.toFloat / math.max(nb_entities,1) + \",\" + \n",
    "                nb_jpa_jdbc.toFloat / math.max(nb_entities,1) + \",\" + \n",
    "                nb_jpa_hbm.toFloat / math.max(nb_entities,1) + \",\" + \n",
    "                nb_jdbc_hbm.toFloat / math.max(nb_entities,1) + \",\" + \n",
    "                nb_jpa_jdbc_hbm.toFloat / math.max(nb_entities,1))\n",
    "    }).mkString(\",\"))\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "output.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
